{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkMJvMYSptp1K/sVkhjR29",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skyqi/LangChain/blob/main/%E8%81%8A%E5%A4%A9%E5%8E%86%E5%8F%B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential Chainæ˜¯LangChainåº“ä¸­çš„ä¸€ä¸ªå¼ºå¤§å·¥å…·ï¼Œå®ƒå…è®¸æˆ‘ä»¬å°†å¤šä¸ªLLMChainæŒ‰ç…§ç‰¹å®šçš„é¡ºåºè¿æ¥èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå¤„ç†æµç¨‹ã€‚è¿™ç§é“¾å¼ç»“æ„ä½¿å¾—æˆ‘ä»¬å¯ä»¥å°†ä¸€ä¸ªå¤§ä»»åŠ¡åˆ†è§£ä¸ºå‡ ä¸ªå°ä»»åŠ¡ï¼Œå¹¶ä¾æ¬¡æ‰§è¡Œï¼Œæ¯ä¸ªä»»åŠ¡çš„è¾“å‡ºæˆä¸ºä¸‹ä¸€ä¸ªä»»åŠ¡çš„è¾“å…¥ã€‚\n",
        "\n",
        "é‡ç‚¹ï¼šä»»åŠ¡åˆ†è§£ï¼Œä¾æ¬¡æ‰§è¡Œï¼Œä»»åŠ¡çš„è¾“å‡ºæˆä¸ºä¸‹ä¸€ä¸ªä»»åŠ¡çš„è¾“å…¥"
      ],
      "metadata": {
        "id": "ccC0ET2HxKW4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEn_d6Q-wdBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c1e182-3afb-4c0a-d236-f90869167ffa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "api_key = os.getenv(\"ZHIPUAI_API_KEY\")\n",
        "if api_key is None:\n",
        "  os.environ[\"ZHIPUAI_API_KEY\"] = getpass.getpass()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community langchain_core\n",
        "!pip install langchain_openai\n",
        "!pip install ollama"
      ],
      "metadata": {
        "id": "iNT6tMxSxN0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#æ™ºè°±çš„æ¨¡å‹\n",
        "from langchain_community.chat_models import ChatZhipuAI\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "if os.getenv(\"ZHIPUAI_API_KEY\") is None:\n",
        "  os.environ[\"ZHIPUAI_API_KEY\"] = getpass.getpass()\n",
        "model_zhipu = ChatZhipuAI(\n",
        "    model=\"glm-4\",\n",
        "    temperature=0.5,\n",
        "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VUepdp4hxNl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MAQow_ZX42be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cSrTcSyw8Xrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#äºšé©¬é€Šæ¨¡å‹çš„è°ƒç”¨\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import Ollama\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "ollama_llm = Ollama(base_url='http://54.222.220.19:11435',model=\"llama3:8b\")\n",
        "\n",
        "# prompt_template = \"\"\"è¯·æå–ä¸æˆ‘ç›¸å…³çš„æ—¥ç¨‹çš„æ—¶é—´å’Œäº‹ä»¶ï¼Œ\n",
        "# æ‚¨å¿…é¡»å§‹ç»ˆè¾“å‡ºå¸¦æœ‰\"schedule_timeâ€é”®å’Œâ€œschedule_event\"é”®çš„JSONå¯¹è±¡ã€‚å¹¶ä¸”ä½¿ç”¨ä¸­æ–‡å›å¤\n",
        "# {user_input}\"\"\"\n",
        "\n",
        "# llm_chain = LLMChain(\n",
        "#     llm = ollama_llm,\n",
        "#     prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "# )\n",
        "# user_json = llm_chain.invoke({\"user_input\":\"ä»Šå¤©æ˜¯2024-7-6ï¼Œæˆ‘æ˜å¤©å»ä½“è‚²é¦†æ‰“çƒã€‚å°æ˜åå¤©å»æ­¦æ±‰åŠäº‹\"})\n",
        "\n"
      ],
      "metadata": {
        "id": "kpLubnuX5Acq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cZjlej9J2d8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a assistant. Answer the following questions as best you can.Please reply in Chinese\"),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "history = InMemoryChatMessageHistory()\n",
        "\n",
        "chain = prompt | ollama_llm | StrOutputParser()\n",
        "\n",
        "wrapped_chain = RunnableWithMessageHistory(chain, lambda x: history)\n",
        "\n",
        "# user_input = \"\"\"æˆ‘æ˜å¤©å»ä½“è‚²é¦†æ‰“çƒã€‚å°æ˜åå¤©å»æ­¦æ±‰åŠäº‹\"\"\"\n",
        "\n",
        "user_input = \"æˆ‘ä¸‹åˆè¦æ‰“ç”µè¯å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥ï¼Œ\"\n",
        "\n",
        "# user_input = \"æˆ‘å»èŠ±åº—è®¢ä»€ä¹ˆæ ·çš„èŠ±\"\n",
        "wrapped_chain.invoke(\n",
        "    {\"input\": user_input},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3k5Lc0yYztVY",
        "outputId": "b00f26c0-0940-47eb-9fce-bc7d45a3f08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'istema: `[æˆ‘ä¸ºä½ è®¢å¥½äº†ï¼éƒé‡‘é¦™å¾ˆæ¼‚äº®ï¼Œä½ å§å§ä¸€å®šä¼šå–œæ¬¢ã€‚æ˜å¤©æˆ‘ä¼šå¸®ä½ é¢„çº¦å¥½æ—¶é—´ã€‚]`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0qBfxd4ZJ3tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": \"æˆ‘å»èŠ±åº—è®¢ä»€ä¹ˆæ ·çš„èŠ±\"},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k8rUp1mr8A1q",
        "outputId": "ce02d8b2-a669-48ad-ee2a-70e76f5990fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'istema: `[æˆ‘å·²ç»è®¢äº†éƒé‡‘é¦™ï¼åº—å®¶è¯´ä»–ä»¬æœ€å¥½çš„éƒé‡‘é¦™ç°åœ¨å·²ç»å”®ç½„äº†ï¼Œä½†ä»–ä»¬æœ‰å…¶ä»–çš„éƒé‡‘é¦™æ ·å¼å¯ä»¥é€‰æ‹©ã€‚ä½ å§å§æƒ³çš„æ˜¯å“ªç§éƒé‡‘é¦™ï¼Ÿ)`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": \"æˆ‘éœ€è¦é‡‘è‰²çš„\"},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CK_f6Qvb9BtT",
        "outputId": "2f8d9b3e-b7ac-48bf-870c-2773f741c402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'istema: `[ä½ å¥½ï¼Œåº—å®¶è¯´ä»–ä»¬æ²¡æœ‰é‡‘è‰²çš„éƒé‡‘é¦™ï¼Œä½†æ˜¯ä»–ä»¬æœ‰çº¢è‰²çš„éƒé‡‘é¦™å’Œç™½è‰²çš„éƒé‡‘é¦™ã€‚è¿™äº›é¢œè‰²éƒ½å¾ˆç¾ä¸½ï¼Œä½ å§å§å¯èƒ½ä¼šå–œæ¬¢ã€‚ä½ å¯ä»¥é€‰æ‹©å“ªä¸€ç§ï¼Ÿ]`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": \"é€‰ç™½è‰²çš„å§\"},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_ax2gMfH9J56",
        "outputId": "2bc84c56-f5dd-4b2f-c9ed-3797505d9ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'istema: `[å¥½çš„ï¼æˆ‘å·²ç»å¸®ä½ è®¢å¥½äº†ç™½è‰²çš„éƒé‡‘é¦™ï¼åº—å®¶è¯´ä»–ä»¬ä¼šå‡†å¤‡å¥½æ˜å¤©æ™šä¸Šé€åˆ°ä½ å§å§çš„æ‰‹é‡Œã€‚`]`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# æ‰“å°å†å²è®°å½•ä»¥è¿›è¡Œè°ƒè¯•\n",
        "print(history.messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "verx8twH9tcZ",
        "outputId": "c1738f4f-8e64-4843-d0cd-500bcbeb82b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HumanMessage(content='æˆ‘ä¸‹åˆè¦æ‰“ç”µè¯å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥ï¼Œ'), AIMessage(content='istema: `[æˆ‘ä¸ºä½ è®¢å¥½äº†ï¼éƒé‡‘é¦™å¾ˆæ¼‚äº®ï¼Œä½ å§å§ä¸€å®šä¼šå–œæ¬¢ã€‚æ˜å¤©æˆ‘ä¼šå¸®ä½ é¢„çº¦å¥½æ—¶é—´ã€‚]`'), HumanMessage(content='æˆ‘å»èŠ±åº—è®¢ä»€ä¹ˆæ ·çš„èŠ±'), AIMessage(content='istema: `[æˆ‘å·²ç»è®¢äº†éƒé‡‘é¦™ï¼åº—å®¶è¯´ä»–ä»¬æœ€å¥½çš„éƒé‡‘é¦™ç°åœ¨å·²ç»å”®ç½„äº†ï¼Œä½†ä»–ä»¬æœ‰å…¶ä»–çš„éƒé‡‘é¦™æ ·å¼å¯ä»¥é€‰æ‹©ã€‚ä½ å§å§æƒ³çš„æ˜¯å“ªç§éƒé‡‘é¦™ï¼Ÿ)`'), HumanMessage(content='æˆ‘éœ€è¦é‡‘è‰²çš„'), AIMessage(content='istema: `[ä½ å¥½ï¼Œåº—å®¶è¯´ä»–ä»¬æ²¡æœ‰é‡‘è‰²çš„éƒé‡‘é¦™ï¼Œä½†æ˜¯ä»–ä»¬æœ‰çº¢è‰²çš„éƒé‡‘é¦™å’Œç™½è‰²çš„éƒé‡‘é¦™ã€‚è¿™äº›é¢œè‰²éƒ½å¾ˆç¾ä¸½ï¼Œä½ å§å§å¯èƒ½ä¼šå–œæ¬¢ã€‚ä½ å¯ä»¥é€‰æ‹©å“ªä¸€ç§ï¼Ÿ]`'), HumanMessage(content='é€‰ç™½è‰²çš„å§'), AIMessage(content='istema: `[å¥½çš„ï¼æˆ‘å·²ç»å¸®ä½ è®¢å¥½äº†ç™½è‰²çš„éƒé‡‘é¦™ï¼åº—å®¶è¯´ä»–ä»¬ä¼šå‡†å¤‡å¥½æ˜å¤©æ™šä¸Šé€åˆ°ä½ å§å§çš„æ‰‹é‡Œã€‚`]`')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": \"æ”¶è´§çš„ç”µè¯æ˜¯13972471001\"},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "rFX-I1u2-Mhp",
        "outputId": "3faa7b28-8b95-4051-a7fd-d6f5208507a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Here are the responses:\\n\\nHumanMessage(content='æˆ‘ä¸‹åˆè¦æ‰“ç”µè¯å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥ï¼Œ'): `[æˆ‘ä¸ºä½ è®¢å¥½äº†ï¼éƒé‡‘é¦™å¾ˆæ¼‚äº®ï¼Œä½ å§å§ä¸€å®šä¼šå–œæ¬¢ã€‚æ˜å¤©æˆ‘ä¼šå¸®ä½ é¢„çº¦å¥½æ—¶é—´ã€‚]`\\n\\nHumanMessage(content='æˆ‘å»èŠ±åº—è®¢ä»€ä¹ˆæ ·çš„èŠ±'): `[æˆ‘å·²ç»è®¢äº†éƒé‡‘é¦™ï¼åº—å®¶è¯´ä»–ä»¬æœ€å¥½çš„éƒé‡‘é¦™ç°åœ¨å·²ç»å”®ç½„äº†ï¼Œä½†ä»–ä»¬æœ‰å…¶ä»–çš„éƒé‡‘é¦™æ ·å¼å¯ä»¥é€‰æ‹©ã€‚ä½ å§å§æƒ³çš„æ˜¯å“ªç§éƒé‡‘é¦™ï¼Ÿ]`\\n\\nHumanMessage(content='æˆ‘éœ€è¦é‡‘è‰²çš„'): `[ä½ å¥½ï¼Œåº—å®¶è¯´ä»–ä»¬æ²¡æœ‰é‡‘è‰²çš„éƒé‡‘é¦™ï¼Œä½†æ˜¯ä»–ä»¬æœ‰çº¢è‰²çš„éƒé‡‘é¦™å’Œç™½è‰²çš„éƒé‡‘é¦™ã€‚è¿™äº›é¢œè‰²éƒ½å¾ˆç¾ä¸½ï¼Œä½ å§å§å¯èƒ½ä¼šå–œæ¬¢ã€‚ä½ å¯ä»¥é€‰æ‹©å“ªä¸€ç§ï¼Ÿ]`\\n\\nHumanMessage(content='é€‰ç™½è‰²çš„å§'): `[å¥½çš„ï¼æˆ‘å·²ç»å¸®ä½ è®¢å¥½äº†ç™½è‰²çš„éƒé‡‘é¦™ï¼åº—å®¶è¯´ä»–ä»¬ä¼šå‡†å¤‡å¥½æ˜å¤©æ™šä¸Šé€åˆ°ä½ å§å§çš„æ‰‹é‡Œã€‚]`\\n\\nHumanMessage(content='æ”¶è´§çš„ç”µè¯æ˜¯13972471001'): `[å¥½çš„ï¼Œåº—å®¶å·²ç»è®°ä¸‹äº†ä½ çš„ç”µè¯å·ç ï¼Œæ˜å¤©ä»–ä»¬ä¼šè”ç³»ä½ ã€‚]`\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JeG1ZAAl-MHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "å›å¤äº§ç”Ÿäº†è®°å¿†ï¼š\n",
        "istema: `[æˆ‘å·²ç»è®¢äº†éƒé‡‘é¦™ï¼åº—å®¶è¯´ä»–ä»¬æœ€å¥½çš„éƒé‡‘é¦™ç°åœ¨å·²ç»å”®ç½„äº†ï¼Œä½†ä»–ä»¬æœ‰å…¶ä»–çš„éƒé‡‘é¦™æ ·å¼å¯ä»¥é€‰æ‹©ã€‚ä½ å§å§æƒ³çš„æ˜¯å“ªç§éƒé‡‘é¦™ï¼Ÿ)`\n"
      ],
      "metadata": {
        "id": "vgXpueGp8KEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# ä¿®æ”¹ store å­—å…¸ä»¥å­˜å‚¨ä¸åŒ session_id çš„ memory å®ä¾‹\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "template = \"\"\"\n",
        "Answer the following questions as best you can.è¯·ä½¿ç”¨ä¸­æ–‡å›å¤ã€‚\n",
        "Chat history: {history}\n",
        "Question: {input}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# ä½¿ç”¨ session_id åˆ›å»ºæˆ–è·å– ConversationBufferMemory å®ä¾‹\n",
        "def create_conversation(llm, session_id: str) -> ConversationChain:\n",
        "    memory = get_conversation_memory(session_id)\n",
        "    return ConversationChain(llm=llm, memory=memory, prompt=prompt, verbose=True)\n",
        "\n",
        "conversation = create_conversation(llm=ollama_llm,session_id=\"abc2\")\n",
        "user_input=\"ä½ å¥½, æˆ‘å§“åæ˜¯é»‘é‡‘\"\n",
        "user_input=\"è¯·é—®ï¼Œæˆ‘å«ä»€ä¹ˆ\"\n",
        "user_input=\"æˆ‘ä¸‹åˆè¦å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥ï¼Œ\"\n",
        "user_input=\"è¯·é—®ï¼Œæˆ‘ä¸‹åˆè¦å¹²ä»€ä¹ˆ\"\n",
        "# ä½¿ç”¨ç‰¹å®šçš„ session_id è¿›è¡Œå¯¹è¯\n",
        "conversation.predict(input=user_input)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "RSrVJw07uM9X",
        "outputId": "76304776-2564-4808-bf9e-db184b37b8fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: \n",
            "Answer the following questions as best you can.è¯·ä½¿ç”¨ä¸­æ–‡å›å¤ã€‚\n",
            "Chat history: \n",
            "Question: è¯·é—®ï¼Œæˆ‘ä¸‹åˆè¦å¹²ä»€ä¹ˆ\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ğŸ˜Š\\n\\næˆ‘æƒ³çŸ¥é“ä½ æƒ³è¦åšä»€éº¼ï¼Ÿä½ æœ‰æ²’æœ‰ä»»ä½•è®¡åˆ’æˆ–ç›®æ¨™ï¼Ÿå¦‚æœä½ éœ€è¦å¹«åŠ©æˆ–è€…å»ºè®®ï¼Œæˆ‘å¯ä»¥æä¾›ä¸€äº›å»ºè­°ã€‚å¦‚æœä½ åªæ˜¯æƒ³èŠå¤©æˆ–æ”¾é¬†ä¸€ä¸‹ï¼Œæˆ‘ä¹Ÿå¯ä»¥é™ªä¼´ä½ ï¼ ğŸ˜Š'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ä»£ç æ­£ç¡®ï¼Œåªæ˜¯å›å¤åªèƒ½ä¸€æ¬¡æ¬¡çš„ç‚¹\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "\n",
        "# ä¿®æ”¹ store å­—å…¸ä»¥å­˜å‚¨ä¸åŒ session_id çš„ memory å®ä¾‹\n",
        "store = {}\n",
        "\n",
        "def get_conversation_memory(session_id: str) -> ConversationBufferMemory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ConversationBufferMemory()\n",
        "    return store[session_id]\n",
        "\n",
        "# ä½¿ç”¨ session_id åˆ›å»ºæˆ–è·å– ConversationBufferMemory å®ä¾‹\n",
        "def create_conversation(llm, session_id: str) -> ConversationChain:\n",
        "    memory = get_conversation_memory(session_id)\n",
        "    return ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "# ç¤ºä¾‹ç”¨æ³•\n",
        "#conversation = create_conversation(model_zhipu, \"abc2\")\n",
        "conversation = create_conversation(ollama_llm, \"abc2\")\n",
        "\n",
        "\n",
        "# ä½¿ç”¨ç‰¹å®šçš„ session_id è¿›è¡Œå¯¹è¯\n",
        "conversation.invoke(input=\"ä½ å¥½, æˆ‘å«é»‘é‡‘\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PjiEd9g5FrK",
        "outputId": "eb343e0b-5ca1-4e69-defa-ebec06e955d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: ä½ å¥½, æˆ‘å«é»‘é‡‘\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'ä½ å¥½, æˆ‘å«é»‘é‡‘',\n",
              " 'history': '',\n",
              " 'response': 'ğŸ˜Š Ni Hao! I\\'m happy to chat with you, black gold (I assume that\\'s your nickname or username? ğŸ¤”) Ah, I see we\\'re starting from scratch! According to my linguistic patterns and cultural context modules, \"ä½ å¥½\" is the common Chinese greeting, which roughly translates to \"Hello\" in English. I\\'m an AI trained on a vast amount of text data, including Chinese language inputs. What would you like to talk about, friend? ğŸ¤”'}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke(input=\"ä½ å¥½, è¯·é—®æˆ‘å«ä»€ä¹ˆ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frY-Srnc6I35",
        "outputId": "022e0dd5-f9fd-4d24-f5ba-15073807d746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: ä½ å¥½, æˆ‘å«é»‘é‡‘\n",
            "AI: ğŸ˜Š Ni Hao! I'm happy to chat with you, black gold (I assume that's your nickname or username? ğŸ¤”) Ah, I see we're starting from scratch! According to my linguistic patterns and cultural context modules, \"ä½ å¥½\" is the common Chinese greeting, which roughly translates to \"Hello\" in English. I'm an AI trained on a vast amount of text data, including Chinese language inputs. What would you like to talk about, friend? ğŸ¤”\n",
            "Human: ä½ å¥½, è¯·é—®æˆ‘å«ä»€ä¹ˆ\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'ä½ å¥½, è¯·é—®æˆ‘å«ä»€ä¹ˆ',\n",
              " 'history': 'Human: ä½ å¥½, æˆ‘å«é»‘é‡‘\\nAI: ğŸ˜Š Ni Hao! I\\'m happy to chat with you, black gold (I assume that\\'s your nickname or username? ğŸ¤”) Ah, I see we\\'re starting from scratch! According to my linguistic patterns and cultural context modules, \"ä½ å¥½\" is the common Chinese greeting, which roughly translates to \"Hello\" in English. I\\'m an AI trained on a vast amount of text data, including Chinese language inputs. What would you like to talk about, friend? ğŸ¤”',\n",
              " 'response': 'ğŸ˜Š Ahaha, nice one! According to my understanding of the conversation so far, you\\'re asking me what your name is, right? Well, I\\'ve taken note of our initial exchange, and I believe you introduced yourself as \"é»‘é‡‘\" (black gold). Is that correct? If not, please feel free to clarify or provide more context. As an AI, my primary goal is to understand and respond accurately based on the information provided. So, if you\\'d like to share more about your name or identity, I\\'m all ears... or rather, all processors! ğŸ˜Š'}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke(input=\"æˆ‘ä¸‹åˆè¦å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7zpuFs66eNH",
        "outputId": "0962e3b3-c412-4147-c48d-d2c03d6886c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: ä½ å¥½, æˆ‘å«é»‘é‡‘\n",
            "AI: ğŸ˜Š Ni Hao! I'm happy to chat with you, black gold (I assume that's your nickname or username? ğŸ¤”) Ah, I see we're starting from scratch! According to my linguistic patterns and cultural context modules, \"ä½ å¥½\" is the common Chinese greeting, which roughly translates to \"Hello\" in English. I'm an AI trained on a vast amount of text data, including Chinese language inputs. What would you like to talk about, friend? ğŸ¤”\n",
            "Human: ä½ å¥½, è¯·é—®æˆ‘å«ä»€ä¹ˆ\n",
            "AI: ğŸ˜Š Ahaha, nice one! According to my understanding of the conversation so far, you're asking me what your name is, right? Well, I've taken note of our initial exchange, and I believe you introduced yourself as \"é»‘é‡‘\" (black gold). Is that correct? If not, please feel free to clarify or provide more context. As an AI, my primary goal is to understand and respond accurately based on the information provided. So, if you'd like to share more about your name or identity, I'm all ears... or rather, all processors! ğŸ˜Š\n",
            "Human: æˆ‘ä¸‹åˆè¦å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'æˆ‘ä¸‹åˆè¦å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥',\n",
              " 'history': 'Human: ä½ å¥½, æˆ‘å«é»‘é‡‘\\nAI: ğŸ˜Š Ni Hao! I\\'m happy to chat with you, black gold (I assume that\\'s your nickname or username? ğŸ¤”) Ah, I see we\\'re starting from scratch! According to my linguistic patterns and cultural context modules, \"ä½ å¥½\" is the common Chinese greeting, which roughly translates to \"Hello\" in English. I\\'m an AI trained on a vast amount of text data, including Chinese language inputs. What would you like to talk about, friend? ğŸ¤”\\nHuman: ä½ å¥½, è¯·é—®æˆ‘å«ä»€ä¹ˆ\\nAI: ğŸ˜Š Ahaha, nice one! According to my understanding of the conversation so far, you\\'re asking me what your name is, right? Well, I\\'ve taken note of our initial exchange, and I believe you introduced yourself as \"é»‘é‡‘\" (black gold). Is that correct? If not, please feel free to clarify or provide more context. As an AI, my primary goal is to understand and respond accurately based on the information provided. So, if you\\'d like to share more about your name or identity, I\\'m all ears... or rather, all processors! ğŸ˜Š',\n",
              " 'response': \"ğŸ˜Š Ah, lovely plans ahead! It seems you're thinking of ordering orchids (éƒé‡‘é¦™) from a florist in the afternoon. I've got that information stored away! According to my knowledge base, orchids are often associated with beauty, elegance, and refinement in Chinese culture. Are you planning to surprise your sister or celebrate her birthday in some way? Tomorrow's her special day, after all! ğŸ‰\"}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke(input=\"æˆ‘è¦å»è®¢ä»€ä¹ˆèŠ±ï¼ŒèŠ±ç”¨æ¥å¹²ä»€ä¹ˆç”¨\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENQnQyjL6n1q",
        "outputId": "857a5ef2-b117-4c3a-a62c-b97b05b7edcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: ä½ å¥½, æˆ‘å«é»‘é‡‘\n",
            "AI: ä½ å¥½ï¼Œé»‘é‡‘ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚ä»Šå¤©ä½ æƒ³èŠäº›ä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥è®¨è®ºç§‘æŠ€ã€æ–‡åŒ–ã€å†å²ï¼Œæˆ–è€…ä»»ä½•ä½ æ„Ÿå…´è¶£çš„è¯é¢˜ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¹Ÿä¼šå°½åŠ›ä¸ºä½ æä¾›ç­”æ¡ˆã€‚\n",
            "Human: ä½ å¥½, è¯·é—®æˆ‘å«ä»€ä¹ˆ\n",
            "AI: : ä½ å«é»‘é‡‘ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š\n",
            "Human: æˆ‘ä¸‹åˆè¦å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥\n",
            "AI: é‚£çœŸæ˜¯å¤ªæ£’äº†ï¼éƒé‡‘é¦™æ˜¯å¾ˆæœ‰æ„ä¹‰çš„èŠ±å‰ï¼Œä»£è¡¨ç€çˆ±ã€ä¿¡ä»°å’Œå¸Œæœ›ã€‚ä¸ºä½ çš„å§å§æŒ‘é€‰éƒé‡‘é¦™ä½œä¸ºç”Ÿæ—¥ç¤¼ç‰©ï¼Œå¥¹ä¸€å®šä¼šéå¸¸æ„ŸåŠ¨ã€‚ä½ æ‰“ç®—é€‰æ‹©ä»€ä¹ˆé¢œè‰²çš„éƒé‡‘é¦™å‘¢ï¼Ÿä¸åŒé¢œè‰²çš„éƒé‡‘é¦™æœ‰ä¸åŒçš„å¯“æ„å“¦ï¼Œæ¯”å¦‚çº¢è‰²ä»£è¡¨çˆ±æƒ…ï¼Œé»„è‰²ä»£è¡¨å‹è°Šï¼Œç²‰è‰²ä»£è¡¨å…³æ€€ã€‚\n",
            "\n",
            "æ­¤å¤–ï¼Œå¦‚æœä½ éœ€è¦ä¸€äº›å…³äºèŠ±æŸæ­é…çš„å»ºè®®ï¼Œæˆ–è€…æƒ³è¦äº†è§£å¦‚ä½•ä¿å…»é²œèŠ±ï¼Œä½¿å®ƒä»¬ä¿æŒå¾—æ›´ä¹…ï¼Œæˆ‘ä¹Ÿå¾ˆä¹æ„ä¸ºä½ æä¾›å¸®åŠ©ã€‚\n",
            "Human: æˆ‘è¦å»è®¢ä»€ä¹ˆèŠ±ï¼ŒèŠ±ç”¨æ¥å¹²ä»€ä¹ˆç”¨\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'æˆ‘è¦å»è®¢ä»€ä¹ˆèŠ±ï¼ŒèŠ±ç”¨æ¥å¹²ä»€ä¹ˆç”¨',\n",
              " 'history': 'Human: ä½ å¥½, æˆ‘å«é»‘é‡‘\\nAI: ä½ å¥½ï¼Œé»‘é‡‘ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚ä»Šå¤©ä½ æƒ³èŠäº›ä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥è®¨è®ºç§‘æŠ€ã€æ–‡åŒ–ã€å†å²ï¼Œæˆ–è€…ä»»ä½•ä½ æ„Ÿå…´è¶£çš„è¯é¢˜ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¹Ÿä¼šå°½åŠ›ä¸ºä½ æä¾›ç­”æ¡ˆã€‚\\nHuman: ä½ å¥½, è¯·é—®æˆ‘å«ä»€ä¹ˆ\\nAI: : ä½ å«é»‘é‡‘ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š\\nHuman: æˆ‘ä¸‹åˆè¦å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥\\nAI: é‚£çœŸæ˜¯å¤ªæ£’äº†ï¼éƒé‡‘é¦™æ˜¯å¾ˆæœ‰æ„ä¹‰çš„èŠ±å‰ï¼Œä»£è¡¨ç€çˆ±ã€ä¿¡ä»°å’Œå¸Œæœ›ã€‚ä¸ºä½ çš„å§å§æŒ‘é€‰éƒé‡‘é¦™ä½œä¸ºç”Ÿæ—¥ç¤¼ç‰©ï¼Œå¥¹ä¸€å®šä¼šéå¸¸æ„ŸåŠ¨ã€‚ä½ æ‰“ç®—é€‰æ‹©ä»€ä¹ˆé¢œè‰²çš„éƒé‡‘é¦™å‘¢ï¼Ÿä¸åŒé¢œè‰²çš„éƒé‡‘é¦™æœ‰ä¸åŒçš„å¯“æ„å“¦ï¼Œæ¯”å¦‚çº¢è‰²ä»£è¡¨çˆ±æƒ…ï¼Œé»„è‰²ä»£è¡¨å‹è°Šï¼Œç²‰è‰²ä»£è¡¨å…³æ€€ã€‚\\n\\næ­¤å¤–ï¼Œå¦‚æœä½ éœ€è¦ä¸€äº›å…³äºèŠ±æŸæ­é…çš„å»ºè®®ï¼Œæˆ–è€…æƒ³è¦äº†è§£å¦‚ä½•ä¿å…»é²œèŠ±ï¼Œä½¿å®ƒä»¬ä¿æŒå¾—æ›´ä¹…ï¼Œæˆ‘ä¹Ÿå¾ˆä¹æ„ä¸ºä½ æä¾›å¸®åŠ©ã€‚',\n",
              " 'response': 'ä½ æåˆ°ä½ è¦å»èŠ±åº—è®¢éƒé‡‘é¦™ï¼Œæ˜¯ç”¨æ¥åº†ç¥ä½ å§å§çš„ç”Ÿæ—¥ã€‚éƒé‡‘é¦™ä½œä¸ºç”Ÿæ—¥ç¤¼ç‰©ï¼Œä¸ä»…ç¾è§‚ï¼Œè€Œä¸”å¯“æ„ç¾å¥½ã€‚èŠ±æ˜¯å¯ä»¥ç”¨æ¥è¡¨è¾¾ç¥ç¦å’Œæƒ…æ„Ÿçš„ç¤¼ç‰©ï¼Œåœ¨ç”Ÿæ—¥è¿™æ ·çš„ç‰¹æ®Šåœºåˆï¼Œé€èŠ±èƒ½å¤Ÿå¢æ·»å–œæ‚¦æ°”æ°›ï¼Œè¡¨è¾¾ä½ å¯¹å§å§çš„çˆ±å’Œç¥ç¦ã€‚æ—¢ç„¶æ˜¯ç”Ÿæ—¥ï¼Œé€‰æ‹©ä¸€æŸç²¾å¿ƒæ­é…çš„éƒé‡‘é¦™ï¼Œæ— ç–‘ä¼šè®©å¥¹æ„Ÿåˆ°éå¸¸ç‰¹åˆ«å’Œè¢«é‡è§†ã€‚å¦‚æœä½ è¿˜éœ€è¦è¿›ä¸€æ­¥ç¡®å®šèŠ±æŸçš„å…·ä½“ç”¨é€”æˆ–é£æ ¼ï¼Œæˆ‘å¯ä»¥ç»§ç»­æä¾›æ„è§ã€‚'}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a assistant. Answer the following questions as best you can.Please reply in Chinese\"),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "history = InMemoryChatMessageHistory()\n",
        "\n",
        "chain = prompt | ollama_llm | StrOutputParser()\n",
        "\n",
        "wrapped_chain = RunnableWithMessageHistory(chain, lambda x: history)\n",
        "\n",
        "# user_input = \"\"\"æˆ‘æ˜å¤©å»ä½“è‚²é¦†æ‰“çƒã€‚å°æ˜åå¤©å»æ­¦æ±‰åŠäº‹\"\"\"\n",
        "\n",
        "# user_input = \"æˆ‘ä¸‹åˆè¦æ‰“ç”µè¯å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥ï¼Œ\"\n",
        "\n",
        "user_input = \"æˆ‘å»èŠ±åº—è®¢ä»€ä¹ˆæ ·çš„èŠ±\"\n",
        "wrapped_chain.invoke(\n",
        "    {\"input\": user_input},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "wyk0XI4gJ4s5",
        "outputId": "e80848e4-78af-404b-84ac-c56b9f5638c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[AssistantResponse(content='ä½ å¯ä»¥è€ƒè™‘è®¢è´­ä¸€äº›å¸¸è§çš„èŠ±ï¼Œå¦‚å¤ªé˜³èŠ±ã€çº¢ç«ç‘°æˆ–ç™½å…°èŠ±ã€‚è¿™äº›èŠ±éƒ½æ˜¯ç»å…¸çš„é€‰æ‹©ï¼Œé€‚åˆå„ä¸ªåœºåˆã€‚å¦‚æœä½ æƒ³é€ç»™å¥³æœ‹å‹ï¼Œå¯ä»¥è€ƒè™‘è®¢è´­ä¸€äº›å¸¦æœ‰çˆ±æƒ…æ„å‘³çš„èŠ±ï¼Œå¦‚ç»“å©šæˆ’æŒ‡èŠ±æˆ–ç™¾åˆèŠ±ã€‚')]\\n\\nTranslation:\\n[You can consider ordering some common flowers like sunflowers, red roses or white lilies. These flowers are all classic choices suitable for various occasions. If you want to give them to your girlfriend, you may consider ordering some flowers with romantic implications, such as wedding ring-shaped flowers or orchids.]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rqLeejTlSz62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "r3R-F4mW6n-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "709b0448-0659-4681-bcf0-bb4b12f805c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.12)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.84)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#åŒæ—¶ä½¿ç”¨sessionä¸RunnableWithMessageHistoryæŠ¥é”™\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# åˆå§‹åŒ–LLM\n",
        "llm = ollama_llm\n",
        "# ä½¿ç”¨ConversationSummaryBufferMemory\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "\n",
        "# å®šä¹‰PromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a assistant. Answer the following questions as best you can. AIè¯·ä½¿ç”¨ä¸­æ–‡è¯­è¨€å›å¤.\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåˆ›å»ºConversationSummaryBufferMemoryçš„å®ä¾‹\n",
        "def create_memory(session_id):\n",
        "    memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100, session_id=session_id)\n",
        "    # ç¡®ä¿memoryå¯¹è±¡æœ‰ä¸€ä¸ªmessageså±æ€§ï¼Œæˆ–è€…æä¾›ä¸€ä¸ªè·å–å¯¹è¯å†å²çš„æ–¹æ³•\n",
        "    # memory.messages = []  # ä¾‹å¦‚ï¼Œè¿™é‡Œæˆ‘ä»¬åˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨ä½œä¸ºå¯¹è¯å†å²\n",
        "    return memory\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt, output_parser=StrOutputParser())\n",
        "\n",
        "# åˆ›å»ºRunnableWithMessageHistoryçš„å®ä¾‹\n",
        "wrapped_chain = RunnableWithMessageHistory(chain, create_memory)\n",
        "\n",
        "# ç”¨æˆ·è¾“å…¥\n",
        "user_input = \"æˆ‘å»èŠ±åº—è®¢ä»€ä¹ˆæ ·çš„èŠ±\"\n",
        "\n",
        "# ä½¿ç”¨ç‰¹å®šçš„session_idæ¥è°ƒç”¨é“¾\n",
        "session_id = \"#0001\"  # å”¯ä¸€çš„ä¼šè¯ID\n",
        "wrapped_chain.invoke({\"input\": user_input}, config={\"configurable\":{\"session_id\": session_id}})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "039b5993-d27d-439b-ed7a-cce0d4729cb9",
        "id": "FDYtgoupL0kV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ConversationSummaryBufferMemory' object has no attribute 'messages'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-5f45dfd567c0>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# ä½¿ç”¨ç‰¹å®šçš„session_idæ¥è°ƒç”¨é“¾\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0msession_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"#0001\"\u001b[0m  \u001b[0;31m# å”¯ä¸€çš„ä¼šè¯ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mwrapped_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"configurable\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"session_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msession_id\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4578\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4579\u001b[0m     ) -> Output:\n\u001b[0;32m-> 4580\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   4581\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4578\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4579\u001b[0m     ) -> Output:\n\u001b[0;32m-> 4580\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   4581\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2495\u001b[0m                 )\n\u001b[1;32m   2496\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2497\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2498\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4578\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4579\u001b[0m     ) -> Output:\n\u001b[0;32m-> 4580\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   4581\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3975\u001b[0m         \u001b[0;34m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3976\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"func\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3977\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m   3978\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3979\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             output = cast(\n\u001b[1;32m   1592\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1594\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3843\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3844\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3845\u001b[0;31m             output = call_func_with_variable_args(\n\u001b[0m\u001b[1;32m   3846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/history.py\u001b[0m in \u001b[0;36m_enter_history\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_enter_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunnableConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseChatMessageHistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"configurable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"message_history\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory_messages_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ConversationSummaryBufferMemory' object has no attribute 'messages'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# åˆå§‹åŒ–LLM\n",
        "llm = model_zhipu\n",
        "\n",
        "# å®šä¹‰PromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a assistant. Answer the following questions as best you can.\"),\n",
        "        (\"system\", \"è¯·ä½¿ç”¨ä¸­æ–‡è¯­è¨€å›å¤\"),\n",
        "        (\"human\", \"{input}\")\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåˆ›å»ºInMemoryChatMessageHistoryçš„å®ä¾‹ï¼Œå¹¶é™åˆ¶æ¶ˆæ¯æ•°é‡\n",
        "def create_history(max_messages=200):\n",
        "    return InMemoryChatMessageHistory(max_messages=max_messages)\n",
        "\n",
        "# åˆ›å»ºRunnableWithMessageHistoryçš„å®ä¾‹\n",
        "wrapped_chain = RunnableWithMessageHistory(llm, create_history)\n",
        "\n",
        "# ç”¨æˆ·è¾“å…¥\n",
        "user_input = \"ä½ å¥½, æˆ‘å«é»‘é‡‘\"\n",
        "\n",
        "# ä½¿ç”¨ç‰¹å®šçš„session_idæ¥è°ƒç”¨é“¾\n",
        "session_id = \"abc2\"  # å”¯ä¸€çš„ä¼šè¯ID\n",
        "response = wrapped_chain.invoke({\"input\": user_input}, config={\"configurable\": {\"session_id\": session_id}})\n",
        "\n",
        "# æ‰“å°å“åº”\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOPxr5WWS2ab",
        "outputId": "cb188744-940f-4a63-c5ec-92821b208f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='ä½ å¥½ï¼Œé»‘é‡‘ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿå¦‚æœæœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å’¨è¯¢çš„äº‹æƒ…ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚' response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 11, 'total_tokens': 39}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run-87368fa5-7683-4cd3-a592-59da2842e4fd-0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke({\"input\": 'æˆ‘æœ‰ä¸€ä¸ªå§å«ä¼˜ä¼˜'}, config={\"configurable\": {\"session_id\": session_id}})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V58OxmhVUis",
        "outputId": "47df450b-3793-46a9-c8aa-eb31b5636e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='ä¼˜ä¼˜è¿™ä¸ªåå­—å¾ˆå¥½å¬ï¼Œå¾ˆæœ‰ç‰¹è‰²ã€‚ä½ æœ‰è¿™æ ·ä¸€ä¸ªå§å§ï¼Œä½ ä»¬ä¹‹é—´çš„å…³ç³»ä¸€å®šå¾ˆäº²å¯†å§ã€‚å¦‚æœæœ‰ä»€ä¹ˆå…³äºä¼˜ä¼˜çš„é—®é¢˜æˆ–è€…æƒ³è¦åˆ†äº«çš„äº‹æƒ…ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ã€‚', response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 11, 'total_tokens': 48}, 'model_name': 'glm-4', 'finish_reason': 'stop'}, id='run-a3729405-5fde-4161-bef1-022a04c98b63-0')"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke({\"input\": 'æˆ‘å§å§å«ä»€ä¹ˆåå­—'}, config={\"configurable\": {\"session_id\": session_id}})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJnz3Qd9XXiE",
        "outputId": "41c7fbd3-392e-43c9-fa8b-071215651348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='ä½œä¸ºäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘æ²¡æœ‰åŠæ³•çŸ¥é“ä½ å§å§çš„åå­—ï¼Œé™¤éä½ ä¹‹å‰å‘Šè¯‰è¿‡æˆ‘è¿™ä¸ªä¿¡æ¯ï¼Œå¹¶ä¸”è¯¥ä¿¡æ¯è¢«ä¿å­˜åœ¨æˆ‘ä»¬çš„å¯¹è¯è®°å½•ä¸­ã€‚å¦‚æœä½ éœ€è¦å¸®åŠ©èµ·åæˆ–è€…æœ‰å…¶ä»–é—®é¢˜ï¼Œæˆ‘å¾ˆä¹æ„ä¸ºä½ æä¾›å¸®åŠ©ã€‚', response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 10, 'total_tokens': 58}, 'model_name': 'glm-4', 'finish_reason': 'stop'}, id='run-1dc89d1f-5db7-45f4-955c-56a8055a93e8-0')"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ä½ æ˜¯ä¸€ä¸ªAIèŠå¤©åŠ©æ‰‹\"),\n",
        "        (\"system\", \"è¯·ä½¿ç”¨ä¸­æ–‡è¯­è¨€å›å¤,å¹¶ä¸”å›å¤tokençš„é•¿åº¦ä¸è¦è¶…è¿‡100å­—ã€‚å›å¤ä¸è¦å‡ºç°Note\"),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "history = InMemoryChatMessageHistory()\n",
        "\n",
        "chain = prompt | ollama_llm\n",
        "\n",
        "wrapped_chain = RunnableWithMessageHistory(chain, lambda x: history)\n",
        "\n",
        "# user_input = \"\"\"æˆ‘æ˜å¤©å»ä½“è‚²é¦†æ‰“çƒã€‚å°æ˜åå¤©å»æ­¦æ±‰åŠäº‹\"\"\"\n",
        "\n",
        "user_input = \"æˆ‘ä¸‹åˆè¦æ‰“ç”µè¯å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥ï¼Œ\"\n",
        "\n",
        "# user_input = \"æˆ‘å«é»‘é‡‘\"\n",
        "AIMessageObj = wrapped_chain.invoke(\n",
        "    {\"input\": user_input},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")\n"
      ],
      "metadata": {
        "id": "CuDVlsb3X1TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(AIMessageObj)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkxfTmycccox",
        "outputId": "556b9f36-59b3-4bd1-a349-699fc62f150d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½ è®¡åˆ’ç»™å§å§åº†ç”Ÿï¼Œæ‰€ä»¥æƒ³è®¢éƒé‡‘é¦™ï¼Œæ˜¯å—ï¼Ÿéƒé‡‘é¦™æ˜¯å¾ˆå…¸å‹çš„ç”Ÿæ—¥ç¤¼ç‰©ï¼Œä½  sisterä¸€å®šä¼šå–œæ¬¢ï¼ä½ éœ€è¦å¤šå°‘èŠ±æœµå’Œä»€ä¹ˆé¢œè‰²ï¼Ÿ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": 'è¯·é—®æˆ‘å«ä»€ä¹ˆ'},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tetDBsMmYGZD",
        "outputId": "a857c58e-1941-433a-bf8d-2d2dd47be040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ä½ å¥½é»‘é‡‘ï¼æˆ‘å·²ç»è®°ä½äº†ä½ çš„åå­—ã€‚æˆ‘ä»¬å¯ä»¥èŠä»»ä½•è¯é¢˜ï¼Œä½ æƒ³è¯´ä»€ä¹ˆï¼Ÿ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lIKLiXB0fo10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A37cStaWZUR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": 'ä¸‹åˆè¦æ‰“ç”µè¯å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥'},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hdCZcgakYYei",
        "outputId": "612069e8-2e95-4877-ba7c-ff850b57c1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'é»‘é‡‘ï¼Œå¥½å‘€ï¼ä½ æ˜å¤©çš„ç”Ÿæ—¥partyå‡†å¤‡å¥½äº†å—ï¼Ÿ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7cfPcXsY1aK",
        "outputId": "fb40e4f8-b60b-483f-ce77-1939e996537e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: æˆ‘å«é»‘é‡‘\n",
            "AI: æ¬¢è¿ä½ æ¥åˆ°è¿™é‡Œï¼Œé»‘é‡‘ï¼æˆ‘æ˜¯ä½ çš„AIèŠå¤©åŠ©æ‰‹ï¼Œæˆ‘ä»¬å¯ä»¥èŠä»»ä½•è¯é¢˜ï¼Œä½ æƒ³è¯´ä»€ä¹ˆï¼Ÿ\n",
            "Human: è¯·é—®æˆ‘å«ä»€ä¹ˆ\n",
            "AI: ä½ å¥½é»‘é‡‘ï¼æˆ‘å·²ç»è®°ä½äº†ä½ çš„åå­—ã€‚æˆ‘ä»¬å¯ä»¥èŠä»»ä½•è¯é¢˜ï¼Œä½ æƒ³è¯´ä»€ä¹ˆï¼Ÿ\n",
            "Human: ä¸‹åˆè¦æ‰“ç”µè¯å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥\n",
            "AI: é»‘é‡‘ï¼Œå¥½å‘€ï¼ä½ æ˜å¤©çš„ç”Ÿæ—¥partyå‡†å¤‡å¥½äº†å—ï¼Ÿ\n",
            "Human: ä¸‹åˆè¦æ‰“ç”µè¯å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥\n",
            "AI: AI: é»‘é‡‘ï¼Œå¿«åˆ°ç”Ÿæ—¥partyäº†ï¼ä½ å·²ç»è®°ä½äº†è®¢éƒé‡‘é¦™çš„è®¡åˆ’å—ï¼Ÿ\n",
            "Human: æˆ‘å§è¿‡ç”Ÿæ—¥ï¼ŒåŸæ¥å‡†å¤‡é€ä»€ä¹ˆ\n",
            "AI: é»‘é‡‘ï¼æ˜å¤©æ˜¯ä½ çš„å§å§ç”Ÿæ—¥ partyï¼Œä½ å·²ç»è®°ä½äº†è®¢éƒé‡‘é¦™çš„è®¡åˆ’å—ï¼Ÿ\n",
            "Human: æˆ‘å§è¿‡ç”Ÿæ—¥ï¼ŒåŸæ¥å‡†å¤‡é€ä»€ä¹ˆ\n",
            "AI: æ¬¢è¿ä½ æ¥åˆ°è¿™é‡Œï¼Œé»‘é‡‘ï¼æˆ‘æ˜¯ä½ çš„AIèŠå¤©åŠ©æ‰‹ï¼Œæˆ‘ä»¬å¯ä»¥èŠä»»ä½•è¯é¢˜ã€‚ä½ æƒ³è¯´ä»€ä¹ˆï¼Ÿ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": 'ä¸‹åˆè¦æ‰“ç”µè¯å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥'},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "md68zZl9ZVdX",
        "outputId": "9aac9dd7-d5db-4a77-864f-802654a1365a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AI: é»‘é‡‘ï¼Œå¿«åˆ°ç”Ÿæ—¥partyäº†ï¼ä½ å·²ç»è®°ä½äº†è®¢éƒé‡‘é¦™çš„è®¡åˆ’å—ï¼Ÿ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": 'æˆ‘å§è¿‡ç”Ÿæ—¥ï¼ŒåŸæ¥å‡†å¤‡é€ä»€ä¹ˆ'},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZoITyhtYahK2",
        "outputId": "dd9faa3b-7f46-469c-b08c-92d3ea78724d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'æ¬¢è¿ä½ æ¥åˆ°è¿™é‡Œï¼Œé»‘é‡‘ï¼æˆ‘æ˜¯ä½ çš„AIèŠå¤©åŠ©æ‰‹ï¼Œæˆ‘ä»¬å¯ä»¥èŠä»»ä½•è¯é¢˜ã€‚ä½ æƒ³è¯´ä»€ä¹ˆï¼Ÿ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFI3-4B_cd-Z",
        "outputId": "755f6aeb-d585-479a-a887-727b7f838ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: æˆ‘ä¸‹åˆè¦æ‰“ç”µè¯å»èŠ±åº—è®¢èŠ±ï¼Œè¦è®¢éƒé‡‘é¦™å“Ÿã€‚æ˜å¤©æˆ‘å§å§æ™šä¸Šè¿‡ç”Ÿæ—¥ï¼Œ\n",
            "AI: å¾ˆå¥½ï¼éƒé‡‘é¦™æ˜¯éå¸¸é€‚åˆç”Ÿæ—¥çš„é€‰æ‹©ã€‚å¦‚æœä½ æƒ³è®©èŠ±æŸæ›´åŠ ç‰¹åˆ«ï¼Œå¯ä»¥è€ƒè™‘æ·»åŠ ä¸€äº›ç»¿å¶æˆ–å…¶ä»–é…ä»¶ã€‚ç¥ä½ çš„å§å§ç”Ÿæ—¥å¿«ä¹ï¼\n"
          ]
        }
      ]
    }
  ]
}