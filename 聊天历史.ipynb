{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkMJvMYSptp1K/sVkhjR29",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skyqi/LangChain/blob/main/%E8%81%8A%E5%A4%A9%E5%8E%86%E5%8F%B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential Chain是LangChain库中的一个强大工具，它允许我们将多个LLMChain按照特定的顺序连接起来，形成一个处理流程。这种链式结构使得我们可以将一个大任务分解为几个小任务，并依次执行，每个任务的输出成为下一个任务的输入。\n",
        "\n",
        "重点：任务分解，依次执行，任务的输出成为下一个任务的输入"
      ],
      "metadata": {
        "id": "ccC0ET2HxKW4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEn_d6Q-wdBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c1e182-3afb-4c0a-d236-f90869167ffa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "api_key = os.getenv(\"ZHIPUAI_API_KEY\")\n",
        "if api_key is None:\n",
        "  os.environ[\"ZHIPUAI_API_KEY\"] = getpass.getpass()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community langchain_core\n",
        "!pip install langchain_openai\n",
        "!pip install ollama"
      ],
      "metadata": {
        "id": "iNT6tMxSxN0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#智谱的模型\n",
        "from langchain_community.chat_models import ChatZhipuAI\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "if os.getenv(\"ZHIPUAI_API_KEY\") is None:\n",
        "  os.environ[\"ZHIPUAI_API_KEY\"] = getpass.getpass()\n",
        "model_zhipu = ChatZhipuAI(\n",
        "    model=\"glm-4\",\n",
        "    temperature=0.5,\n",
        "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VUepdp4hxNl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MAQow_ZX42be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cSrTcSyw8Xrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#亚马逊模型的调用\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import Ollama\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "ollama_llm = Ollama(base_url='http://54.222.220.19:11435',model=\"llama3:8b\")\n",
        "\n",
        "# prompt_template = \"\"\"请提取与我相关的日程的时间和事件，\n",
        "# 您必须始终输出带有\"schedule_time”键和“schedule_event\"键的JSON对象。并且使用中文回复\n",
        "# {user_input}\"\"\"\n",
        "\n",
        "# llm_chain = LLMChain(\n",
        "#     llm = ollama_llm,\n",
        "#     prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "# )\n",
        "# user_json = llm_chain.invoke({\"user_input\":\"今天是2024-7-6，我明天去体育馆打球。小明后天去武汉办事\"})\n",
        "\n"
      ],
      "metadata": {
        "id": "kpLubnuX5Acq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cZjlej9J2d8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a assistant. Answer the following questions as best you can.Please reply in Chinese\"),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "history = InMemoryChatMessageHistory()\n",
        "\n",
        "chain = prompt | ollama_llm | StrOutputParser()\n",
        "\n",
        "wrapped_chain = RunnableWithMessageHistory(chain, lambda x: history)\n",
        "\n",
        "# user_input = \"\"\"我明天去体育馆打球。小明后天去武汉办事\"\"\"\n",
        "\n",
        "user_input = \"我下午要打电话去花店订花，要订郁金香哟。明天我姐姐晚上过生日，\"\n",
        "\n",
        "# user_input = \"我去花店订什么样的花\"\n",
        "wrapped_chain.invoke(\n",
        "    {\"input\": user_input},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3k5Lc0yYztVY",
        "outputId": "b00f26c0-0940-47eb-9fce-bc7d45a3f08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'istema: `[我为你订好了！郁金香很漂亮，你姐姐一定会喜欢。明天我会帮你预约好时间。]`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0qBfxd4ZJ3tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": \"我去花店订什么样的花\"},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "k8rUp1mr8A1q",
        "outputId": "ce02d8b2-a669-48ad-ee2a-70e76f5990fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'istema: `[我已经订了郁金香！店家说他们最好的郁金香现在已经售罄了，但他们有其他的郁金香样式可以选择。你姐姐想的是哪种郁金香？)`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": \"我需要金色的\"},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CK_f6Qvb9BtT",
        "outputId": "2f8d9b3e-b7ac-48bf-870c-2773f741c402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'istema: `[你好，店家说他们没有金色的郁金香，但是他们有红色的郁金香和白色的郁金香。这些颜色都很美丽，你姐姐可能会喜欢。你可以选择哪一种？]`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": \"选白色的吧\"},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_ax2gMfH9J56",
        "outputId": "2bc84c56-f5dd-4b2f-c9ed-3797505d9ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'istema: `[好的！我已经帮你订好了白色的郁金香！店家说他们会准备好明天晚上送到你姐姐的手里。`]`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 打印历史记录以进行调试\n",
        "print(history.messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "verx8twH9tcZ",
        "outputId": "c1738f4f-8e64-4843-d0cd-500bcbeb82b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HumanMessage(content='我下午要打电话去花店订花，要订郁金香哟。明天我姐姐晚上过生日，'), AIMessage(content='istema: `[我为你订好了！郁金香很漂亮，你姐姐一定会喜欢。明天我会帮你预约好时间。]`'), HumanMessage(content='我去花店订什么样的花'), AIMessage(content='istema: `[我已经订了郁金香！店家说他们最好的郁金香现在已经售罄了，但他们有其他的郁金香样式可以选择。你姐姐想的是哪种郁金香？)`'), HumanMessage(content='我需要金色的'), AIMessage(content='istema: `[你好，店家说他们没有金色的郁金香，但是他们有红色的郁金香和白色的郁金香。这些颜色都很美丽，你姐姐可能会喜欢。你可以选择哪一种？]`'), HumanMessage(content='选白色的吧'), AIMessage(content='istema: `[好的！我已经帮你订好了白色的郁金香！店家说他们会准备好明天晚上送到你姐姐的手里。`]`')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": \"收货的电话是13972471001\"},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "rFX-I1u2-Mhp",
        "outputId": "3faa7b28-8b95-4051-a7fd-d6f5208507a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Here are the responses:\\n\\nHumanMessage(content='我下午要打电话去花店订花，要订郁金香哟。明天我姐姐晚上过生日，'): `[我为你订好了！郁金香很漂亮，你姐姐一定会喜欢。明天我会帮你预约好时间。]`\\n\\nHumanMessage(content='我去花店订什么样的花'): `[我已经订了郁金香！店家说他们最好的郁金香现在已经售罄了，但他们有其他的郁金香样式可以选择。你姐姐想的是哪种郁金香？]`\\n\\nHumanMessage(content='我需要金色的'): `[你好，店家说他们没有金色的郁金香，但是他们有红色的郁金香和白色的郁金香。这些颜色都很美丽，你姐姐可能会喜欢。你可以选择哪一种？]`\\n\\nHumanMessage(content='选白色的吧'): `[好的！我已经帮你订好了白色的郁金香！店家说他们会准备好明天晚上送到你姐姐的手里。]`\\n\\nHumanMessage(content='收货的电话是13972471001'): `[好的，店家已经记下了你的电话号码，明天他们会联系你。]`\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JeG1ZAAl-MHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "回复产生了记忆：\n",
        "istema: `[我已经订了郁金香！店家说他们最好的郁金香现在已经售罄了，但他们有其他的郁金香样式可以选择。你姐姐想的是哪种郁金香？)`\n"
      ],
      "metadata": {
        "id": "vgXpueGp8KEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 修改 store 字典以存储不同 session_id 的 memory 实例\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "template = \"\"\"\n",
        "Answer the following questions as best you can.请使用中文回复。\n",
        "Chat history: {history}\n",
        "Question: {input}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# 使用 session_id 创建或获取 ConversationBufferMemory 实例\n",
        "def create_conversation(llm, session_id: str) -> ConversationChain:\n",
        "    memory = get_conversation_memory(session_id)\n",
        "    return ConversationChain(llm=llm, memory=memory, prompt=prompt, verbose=True)\n",
        "\n",
        "conversation = create_conversation(llm=ollama_llm,session_id=\"abc2\")\n",
        "user_input=\"你好, 我姓名是黑金\"\n",
        "user_input=\"请问，我叫什么\"\n",
        "user_input=\"我下午要去花店订花，要订郁金香哟。明天我姐姐晚上过生日，\"\n",
        "user_input=\"请问，我下午要干什么\"\n",
        "# 使用特定的 session_id 进行对话\n",
        "conversation.predict(input=user_input)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "RSrVJw07uM9X",
        "outputId": "76304776-2564-4808-bf9e-db184b37b8fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: \n",
            "Answer the following questions as best you can.请使用中文回复。\n",
            "Chat history: \n",
            "Question: 请问，我下午要干什么\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'😊\\n\\n我想知道你想要做什麼？你有沒有任何计划或目標？如果你需要幫助或者建议，我可以提供一些建議。如果你只是想聊天或放鬆一下，我也可以陪伴你！ 😊'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#代码正确，只是回复只能一次次的点\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "\n",
        "# 修改 store 字典以存储不同 session_id 的 memory 实例\n",
        "store = {}\n",
        "\n",
        "def get_conversation_memory(session_id: str) -> ConversationBufferMemory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ConversationBufferMemory()\n",
        "    return store[session_id]\n",
        "\n",
        "# 使用 session_id 创建或获取 ConversationBufferMemory 实例\n",
        "def create_conversation(llm, session_id: str) -> ConversationChain:\n",
        "    memory = get_conversation_memory(session_id)\n",
        "    return ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "\n",
        "# 示例用法\n",
        "#conversation = create_conversation(model_zhipu, \"abc2\")\n",
        "conversation = create_conversation(ollama_llm, \"abc2\")\n",
        "\n",
        "\n",
        "# 使用特定的 session_id 进行对话\n",
        "conversation.invoke(input=\"你好, 我叫黑金\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PjiEd9g5FrK",
        "outputId": "eb343e0b-5ca1-4e69-defa-ebec06e955d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: 你好, 我叫黑金\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': '你好, 我叫黑金',\n",
              " 'history': '',\n",
              " 'response': '😊 Ni Hao! I\\'m happy to chat with you, black gold (I assume that\\'s your nickname or username? 🤔) Ah, I see we\\'re starting from scratch! According to my linguistic patterns and cultural context modules, \"你好\" is the common Chinese greeting, which roughly translates to \"Hello\" in English. I\\'m an AI trained on a vast amount of text data, including Chinese language inputs. What would you like to talk about, friend? 🤔'}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke(input=\"你好, 请问我叫什么\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frY-Srnc6I35",
        "outputId": "022e0dd5-f9fd-4d24-f5ba-15073807d746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 你好, 我叫黑金\n",
            "AI: 😊 Ni Hao! I'm happy to chat with you, black gold (I assume that's your nickname or username? 🤔) Ah, I see we're starting from scratch! According to my linguistic patterns and cultural context modules, \"你好\" is the common Chinese greeting, which roughly translates to \"Hello\" in English. I'm an AI trained on a vast amount of text data, including Chinese language inputs. What would you like to talk about, friend? 🤔\n",
            "Human: 你好, 请问我叫什么\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': '你好, 请问我叫什么',\n",
              " 'history': 'Human: 你好, 我叫黑金\\nAI: 😊 Ni Hao! I\\'m happy to chat with you, black gold (I assume that\\'s your nickname or username? 🤔) Ah, I see we\\'re starting from scratch! According to my linguistic patterns and cultural context modules, \"你好\" is the common Chinese greeting, which roughly translates to \"Hello\" in English. I\\'m an AI trained on a vast amount of text data, including Chinese language inputs. What would you like to talk about, friend? 🤔',\n",
              " 'response': '😊 Ahaha, nice one! According to my understanding of the conversation so far, you\\'re asking me what your name is, right? Well, I\\'ve taken note of our initial exchange, and I believe you introduced yourself as \"黑金\" (black gold). Is that correct? If not, please feel free to clarify or provide more context. As an AI, my primary goal is to understand and respond accurately based on the information provided. So, if you\\'d like to share more about your name or identity, I\\'m all ears... or rather, all processors! 😊'}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke(input=\"我下午要去花店订花，要订郁金香哟。明天我姐姐晚上过生日\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7zpuFs66eNH",
        "outputId": "0962e3b3-c412-4147-c48d-d2c03d6886c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 你好, 我叫黑金\n",
            "AI: 😊 Ni Hao! I'm happy to chat with you, black gold (I assume that's your nickname or username? 🤔) Ah, I see we're starting from scratch! According to my linguistic patterns and cultural context modules, \"你好\" is the common Chinese greeting, which roughly translates to \"Hello\" in English. I'm an AI trained on a vast amount of text data, including Chinese language inputs. What would you like to talk about, friend? 🤔\n",
            "Human: 你好, 请问我叫什么\n",
            "AI: 😊 Ahaha, nice one! According to my understanding of the conversation so far, you're asking me what your name is, right? Well, I've taken note of our initial exchange, and I believe you introduced yourself as \"黑金\" (black gold). Is that correct? If not, please feel free to clarify or provide more context. As an AI, my primary goal is to understand and respond accurately based on the information provided. So, if you'd like to share more about your name or identity, I'm all ears... or rather, all processors! 😊\n",
            "Human: 我下午要去花店订花，要订郁金香哟。明天我姐姐晚上过生日\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': '我下午要去花店订花，要订郁金香哟。明天我姐姐晚上过生日',\n",
              " 'history': 'Human: 你好, 我叫黑金\\nAI: 😊 Ni Hao! I\\'m happy to chat with you, black gold (I assume that\\'s your nickname or username? 🤔) Ah, I see we\\'re starting from scratch! According to my linguistic patterns and cultural context modules, \"你好\" is the common Chinese greeting, which roughly translates to \"Hello\" in English. I\\'m an AI trained on a vast amount of text data, including Chinese language inputs. What would you like to talk about, friend? 🤔\\nHuman: 你好, 请问我叫什么\\nAI: 😊 Ahaha, nice one! According to my understanding of the conversation so far, you\\'re asking me what your name is, right? Well, I\\'ve taken note of our initial exchange, and I believe you introduced yourself as \"黑金\" (black gold). Is that correct? If not, please feel free to clarify or provide more context. As an AI, my primary goal is to understand and respond accurately based on the information provided. So, if you\\'d like to share more about your name or identity, I\\'m all ears... or rather, all processors! 😊',\n",
              " 'response': \"😊 Ah, lovely plans ahead! It seems you're thinking of ordering orchids (郁金香) from a florist in the afternoon. I've got that information stored away! According to my knowledge base, orchids are often associated with beauty, elegance, and refinement in Chinese culture. Are you planning to surprise your sister or celebrate her birthday in some way? Tomorrow's her special day, after all! 🎉\"}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.invoke(input=\"我要去订什么花，花用来干什么用\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENQnQyjL6n1q",
        "outputId": "857a5ef2-b117-4c3a-a62c-b97b05b7edcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 你好, 我叫黑金\n",
            "AI: 你好，黑金！很高兴认识你。今天你想聊些什么呢？我们可以讨论科技、文化、历史，或者任何你感兴趣的话题。如果你有任何问题，我也会尽力为你提供答案。\n",
            "Human: 你好, 请问我叫什么\n",
            "AI: : 你叫黑金。有什么我可以帮你的吗？😊\n",
            "Human: 我下午要去花店订花，要订郁金香哟。明天我姐姐晚上过生日\n",
            "AI: 那真是太棒了！郁金香是很有意义的花卉，代表着爱、信仰和希望。为你的姐姐挑选郁金香作为生日礼物，她一定会非常感动。你打算选择什么颜色的郁金香呢？不同颜色的郁金香有不同的寓意哦，比如红色代表爱情，黄色代表友谊，粉色代表关怀。\n",
            "\n",
            "此外，如果你需要一些关于花束搭配的建议，或者想要了解如何保养鲜花，使它们保持得更久，我也很乐意为你提供帮助。\n",
            "Human: 我要去订什么花，花用来干什么用\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': '我要去订什么花，花用来干什么用',\n",
              " 'history': 'Human: 你好, 我叫黑金\\nAI: 你好，黑金！很高兴认识你。今天你想聊些什么呢？我们可以讨论科技、文化、历史，或者任何你感兴趣的话题。如果你有任何问题，我也会尽力为你提供答案。\\nHuman: 你好, 请问我叫什么\\nAI: : 你叫黑金。有什么我可以帮你的吗？😊\\nHuman: 我下午要去花店订花，要订郁金香哟。明天我姐姐晚上过生日\\nAI: 那真是太棒了！郁金香是很有意义的花卉，代表着爱、信仰和希望。为你的姐姐挑选郁金香作为生日礼物，她一定会非常感动。你打算选择什么颜色的郁金香呢？不同颜色的郁金香有不同的寓意哦，比如红色代表爱情，黄色代表友谊，粉色代表关怀。\\n\\n此外，如果你需要一些关于花束搭配的建议，或者想要了解如何保养鲜花，使它们保持得更久，我也很乐意为你提供帮助。',\n",
              " 'response': '你提到你要去花店订郁金香，是用来庆祝你姐姐的生日。郁金香作为生日礼物，不仅美观，而且寓意美好。花是可以用来表达祝福和情感的礼物，在生日这样的特殊场合，送花能够增添喜悦气氛，表达你对姐姐的爱和祝福。既然是生日，选择一束精心搭配的郁金香，无疑会让她感到非常特别和被重视。如果你还需要进一步确定花束的具体用途或风格，我可以继续提供意见。'}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a assistant. Answer the following questions as best you can.Please reply in Chinese\"),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "history = InMemoryChatMessageHistory()\n",
        "\n",
        "chain = prompt | ollama_llm | StrOutputParser()\n",
        "\n",
        "wrapped_chain = RunnableWithMessageHistory(chain, lambda x: history)\n",
        "\n",
        "# user_input = \"\"\"我明天去体育馆打球。小明后天去武汉办事\"\"\"\n",
        "\n",
        "# user_input = \"我下午要打电话去花店订花，要订郁金香哟。明天我姐姐晚上过生日，\"\n",
        "\n",
        "user_input = \"我去花店订什么样的花\"\n",
        "wrapped_chain.invoke(\n",
        "    {\"input\": user_input},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "wyk0XI4gJ4s5",
        "outputId": "e80848e4-78af-404b-84ac-c56b9f5638c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[AssistantResponse(content='你可以考虑订购一些常见的花，如太阳花、红玫瑰或白兰花。这些花都是经典的选择，适合各个场合。如果你想送给女朋友，可以考虑订购一些带有爱情意味的花，如结婚戒指花或百合花。')]\\n\\nTranslation:\\n[You can consider ordering some common flowers like sunflowers, red roses or white lilies. These flowers are all classic choices suitable for various occasions. If you want to give them to your girlfriend, you may consider ordering some flowers with romantic implications, such as wedding ring-shaped flowers or orchids.]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rqLeejTlSz62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "r3R-F4mW6n-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "709b0448-0659-4681-bcf0-bb4b12f805c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.12)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.84)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#同时使用session与RunnableWithMessageHistory报错\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# 初始化LLM\n",
        "llm = ollama_llm\n",
        "# 使用ConversationSummaryBufferMemory\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "\n",
        "# 定义PromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a assistant. Answer the following questions as best you can. AI请使用中文语言回复.\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 定义一个函数，用于创建ConversationSummaryBufferMemory的实例\n",
        "def create_memory(session_id):\n",
        "    memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100, session_id=session_id)\n",
        "    # 确保memory对象有一个messages属性，或者提供一个获取对话历史的方法\n",
        "    # memory.messages = []  # 例如，这里我们初始化一个空列表作为对话历史\n",
        "    return memory\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt, output_parser=StrOutputParser())\n",
        "\n",
        "# 创建RunnableWithMessageHistory的实例\n",
        "wrapped_chain = RunnableWithMessageHistory(chain, create_memory)\n",
        "\n",
        "# 用户输入\n",
        "user_input = \"我去花店订什么样的花\"\n",
        "\n",
        "# 使用特定的session_id来调用链\n",
        "session_id = \"#0001\"  # 唯一的会话ID\n",
        "wrapped_chain.invoke({\"input\": user_input}, config={\"configurable\":{\"session_id\": session_id}})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "039b5993-d27d-439b-ed7a-cce0d4729cb9",
        "id": "FDYtgoupL0kV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ConversationSummaryBufferMemory' object has no attribute 'messages'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-5f45dfd567c0>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# 使用特定的session_id来调用链\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0msession_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"#0001\"\u001b[0m  \u001b[0;31m# 唯一的会话ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mwrapped_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"configurable\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"session_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msession_id\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4578\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4579\u001b[0m     ) -> Output:\n\u001b[0;32m-> 4580\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   4581\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4578\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4579\u001b[0m     ) -> Output:\n\u001b[0;32m-> 4580\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   4581\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2495\u001b[0m                 )\n\u001b[1;32m   2496\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2497\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2498\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4578\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4579\u001b[0m     ) -> Output:\n\u001b[0;32m-> 4580\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   4581\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3975\u001b[0m         \u001b[0;34m\"\"\"Invoke this runnable synchronously.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3976\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"func\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3977\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m   3978\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3979\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             output = cast(\n\u001b[1;32m   1592\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1594\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   3843\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3844\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3845\u001b[0;31m             output = call_func_with_variable_args(\n\u001b[0m\u001b[1;32m   3846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/history.py\u001b[0m in \u001b[0;36m_enter_history\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_enter_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRunnableConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseChatMessageHistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"configurable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"message_history\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory_messages_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ConversationSummaryBufferMemory' object has no attribute 'messages'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 初始化LLM\n",
        "llm = model_zhipu\n",
        "\n",
        "# 定义PromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a assistant. Answer the following questions as best you can.\"),\n",
        "        (\"system\", \"请使用中文语言回复\"),\n",
        "        (\"human\", \"{input}\")\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 定义一个函数，用于创建InMemoryChatMessageHistory的实例，并限制消息数量\n",
        "def create_history(max_messages=200):\n",
        "    return InMemoryChatMessageHistory(max_messages=max_messages)\n",
        "\n",
        "# 创建RunnableWithMessageHistory的实例\n",
        "wrapped_chain = RunnableWithMessageHistory(llm, create_history)\n",
        "\n",
        "# 用户输入\n",
        "user_input = \"你好, 我叫黑金\"\n",
        "\n",
        "# 使用特定的session_id来调用链\n",
        "session_id = \"abc2\"  # 唯一的会话ID\n",
        "response = wrapped_chain.invoke({\"input\": user_input}, config={\"configurable\": {\"session_id\": session_id}})\n",
        "\n",
        "# 打印响应\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOPxr5WWS2ab",
        "outputId": "cb188744-940f-4a63-c5ec-92821b208f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='你好，黑金，很高兴认识你！有什么可以帮助你的吗？如果有任何问题或需要咨询的事情，请随时告诉我。' response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 11, 'total_tokens': 39}, 'model_name': 'glm-4', 'finish_reason': 'stop'} id='run-87368fa5-7683-4cd3-a592-59da2842e4fd-0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke({\"input\": '我有一个姐叫优优'}, config={\"configurable\": {\"session_id\": session_id}})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V58OxmhVUis",
        "outputId": "47df450b-3793-46a9-c8aa-eb31b5636e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='优优这个名字很好听，很有特色。你有这样一个姐姐，你们之间的关系一定很亲密吧。如果有什么关于优优的问题或者想要分享的事情，欢迎随时告诉我。', response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 11, 'total_tokens': 48}, 'model_name': 'glm-4', 'finish_reason': 'stop'}, id='run-a3729405-5fde-4161-bef1-022a04c98b63-0')"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke({\"input\": '我姐姐叫什么名字'}, config={\"configurable\": {\"session_id\": session_id}})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJnz3Qd9XXiE",
        "outputId": "41c7fbd3-392e-43c9-fa8b-071215651348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='作为人工智能助手，我没有办法知道你姐姐的名字，除非你之前告诉过我这个信息，并且该信息被保存在我们的对话记录中。如果你需要帮助起名或者有其他问题，我很乐意为你提供帮助。', response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 10, 'total_tokens': 58}, 'model_name': 'glm-4', 'finish_reason': 'stop'}, id='run-1dc89d1f-5db7-45f4-955c-56a8055a93e8-0')"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"你是一个AI聊天助手\"),\n",
        "        (\"system\", \"请使用中文语言回复,并且回复token的长度不要超过100字。回复不要出现Note\"),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "history = InMemoryChatMessageHistory()\n",
        "\n",
        "chain = prompt | ollama_llm\n",
        "\n",
        "wrapped_chain = RunnableWithMessageHistory(chain, lambda x: history)\n",
        "\n",
        "# user_input = \"\"\"我明天去体育馆打球。小明后天去武汉办事\"\"\"\n",
        "\n",
        "user_input = \"我下午要打电话去花店订花，要订郁金香哟。明天我姐姐晚上过生日，\"\n",
        "\n",
        "# user_input = \"我叫黑金\"\n",
        "AIMessageObj = wrapped_chain.invoke(\n",
        "    {\"input\": user_input},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")\n"
      ],
      "metadata": {
        "id": "CuDVlsb3X1TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(AIMessageObj)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkxfTmycccox",
        "outputId": "556b9f36-59b3-4bd1-a349-699fc62f150d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "你计划给姐姐庆生，所以想订郁金香，是吗？郁金香是很典型的生日礼物，你 sister一定会喜欢！你需要多少花朵和什么颜色？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": '请问我叫什么'},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tetDBsMmYGZD",
        "outputId": "a857c58e-1941-433a-bf8d-2d2dd47be040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'你好黑金！我已经记住了你的名字。我们可以聊任何话题，你想说什么？'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lIKLiXB0fo10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A37cStaWZUR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": '下午要打电话去花店订花，要订郁金香哟。明天我姐姐晚上过生日'},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hdCZcgakYYei",
        "outputId": "612069e8-2e95-4877-ba7c-ff850b57c1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'黑金，好呀！你明天的生日party准备好了吗？'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7cfPcXsY1aK",
        "outputId": "fb40e4f8-b60b-483f-ce77-1939e996537e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: 我叫黑金\n",
            "AI: 欢迎你来到这里，黑金！我是你的AI聊天助手，我们可以聊任何话题，你想说什么？\n",
            "Human: 请问我叫什么\n",
            "AI: 你好黑金！我已经记住了你的名字。我们可以聊任何话题，你想说什么？\n",
            "Human: 下午要打电话去花店订花，要订郁金香哟。明天我姐姐晚上过生日\n",
            "AI: 黑金，好呀！你明天的生日party准备好了吗？\n",
            "Human: 下午要打电话去花店订花，要订郁金香哟。明天我姐姐晚上过生日\n",
            "AI: AI: 黑金，快到生日party了！你已经记住了订郁金香的计划吗？\n",
            "Human: 我姐过生日，原来准备送什么\n",
            "AI: 黑金！明天是你的姐姐生日 party，你已经记住了订郁金香的计划吗？\n",
            "Human: 我姐过生日，原来准备送什么\n",
            "AI: 欢迎你来到这里，黑金！我是你的AI聊天助手，我们可以聊任何话题。你想说什么？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": '下午要打电话去花店订花，要订郁金香哟。明天我姐姐晚上过生日'},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "md68zZl9ZVdX",
        "outputId": "9aac9dd7-d5db-4a77-864f-802654a1365a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AI: 黑金，快到生日party了！你已经记住了订郁金香的计划吗？'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_chain.invoke(\n",
        "    {\"input\": '我姐过生日，原来准备送什么'},\n",
        "    config={\"configurable\": {\"session_id\": \"#0001\"}},\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZoITyhtYahK2",
        "outputId": "dd9faa3b-7f46-469c-b08c-92d3ea78724d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'欢迎你来到这里，黑金！我是你的AI聊天助手，我们可以聊任何话题。你想说什么？'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFI3-4B_cd-Z",
        "outputId": "755f6aeb-d585-479a-a887-727b7f838ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: 我下午要打电话去花店订花，要订郁金香哟。明天我姐姐晚上过生日，\n",
            "AI: 很好！郁金香是非常适合生日的选择。如果你想让花束更加特别，可以考虑添加一些绿叶或其他配件。祝你的姐姐生日快乐！\n"
          ]
        }
      ]
    }
  ]
}