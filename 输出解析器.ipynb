{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlke7T51J5gyLCLXBHUNPw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skyqi/LangChain/blob/main/%E8%BE%93%E5%87%BA%E8%A7%A3%E6%9E%90%E5%99%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential Chain是LangChain库中的一个强大工具，它允许我们将多个LLMChain按照特定的顺序连接起来，形成一个处理流程。这种链式结构使得我们可以将一个大任务分解为几个小任务，并依次执行，每个任务的输出成为下一个任务的输入。\n",
        "\n",
        "重点：任务分解，依次执行，任务的输出成为下一个任务的输入"
      ],
      "metadata": {
        "id": "ccC0ET2HxKW4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEn_d6Q-wdBI",
        "outputId": "5f82970c-9a11-4022-a8d7-0656bdf5d290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import getpass\n",
        "\n",
        "# api_key = os.getenv(\"ZHIPUAI_API_KEY\")\n",
        "# if api_key is None:\n",
        "#   os.environ[\"ZHIPUAI_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RgKNqKPg_vLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community langchain_core\n"
      ],
      "metadata": {
        "id": "iNT6tMxSxN0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade 'openai>=1.0'\n",
        "!pip install langchain_openai\n",
        "!pip install ollama"
      ],
      "metadata": {
        "id": "Fc_TjsLp0o3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatZhipuAI\n",
        "import os\n",
        "import getpass\n",
        "os.environ[\"ZHIPUAI_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "model = ChatZhipuAI(\n",
        "    model=\"glm-4\",\n",
        "    temperature=0.5,\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VUepdp4hxNl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56d8a50-0155-4f8c-d04f-950596a74c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#报错\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm_ollam = ChatOpenAI(\n",
        "    temperature=0.95,\n",
        "    model=\"llama3:8b\",\n",
        "    openai_api_key=\"api-key\",\n",
        "    openai_api_base=\"http://54.222.220.19:11435/api/chat\"\n",
        ")\n",
        "# 构建提示\n",
        "prompt = \"你好,我是史帝文\"\n",
        "\n",
        "# 使用模型生成响应\n",
        "response = llm_ollam.invoke(prompt)\n",
        "\n",
        "# 打印响应\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "oeLV9IC5xN2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install ollama\n",
        "from langchain_core.output_parsers import JSONOutputParser\n",
        "from langchain_core.chat_models import ChatModel\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 假设我们已经有了一个配置好的ChatModel实例\n",
        "# model = ...\n",
        "\n",
        "# 创建一个提示模板\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    ('system', 'You are a helpful assistant.'),\n",
        "    ('user', 'What is the capital of France?')\n",
        "])\n",
        "\n",
        "# 创建JSON输出解析器实例\n",
        "# 指定预期的JSON结构\n",
        "json_parser = JSONOutputParser(expected_keys={'answer': str, 'additional_info': str})\n",
        "\n",
        "# 使用模型生成响应\n",
        "response = model.generate_response(prompt_template)\n",
        "\n",
        "# 使用JSON输出解析器解析输出\n",
        "parsed_json = json_parser.parse(response)\n",
        "\n",
        "print(parsed_json)\n"
      ],
      "metadata": {
        "id": "3KCra4Wq17IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#新的代码\n",
        "#!pip install -qU langchain-openai\n",
        "!pip install ollama"
      ],
      "metadata": {
        "id": "H8IQm6YCxN5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "# model38b = ChatOpenAI(\n",
        "#     temperature=0.95,\n",
        "#     model=\"llama3:8b\",\n",
        "#     openai_api_key=\"api-key\",\n",
        "#     openai_api_base=\"http://54.222.220.19:11435/api/chat\"\n",
        "# )\n",
        "\n",
        "from langchain.llms import ollama\n",
        "ollama = ollama(base_url='http://54.222.220.19:11435/api/chat',model=\"llama3:8b\")\n",
        "ollama(\"why is the sky blue\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "MAQow_ZX42be",
        "outputId": "f016c6b5-96d7-43a6-a3e8-aa58b3307993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'module' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-542326caf2b1>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mollama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mollama\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mollama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'http://54.222.220.19:11435/api/chat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama3:8b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mollama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"why is the sky blue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#亚马逊模型的调用\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import Ollama\n",
        "\n",
        "prompt_template = \"What is a good name for a company that makes {product}?\"\n",
        "\n",
        "ollama_llm = Ollama(base_url='http://54.222.220.19:11435',model=\"llama3:8b\")\n",
        "llm_chain = LLMChain(\n",
        "    llm = ollama_llm,\n",
        "    prompt = PromptTemplate.from_template(prompt_template)\n",
        ")\n",
        "print(llm_chain(\"colorful socks\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al9K7XA85AuG",
        "outputId": "c9636b40-b357-4a18-e9e6-d28437c15ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'product': 'colorful socks', 'text': 'What a fun question!\\n\\nHere are some name suggestions for a company that makes colorful socks:\\n\\n1. **Sockful of Joy**: This name conveys the idea that your socks will bring happiness to those who wear them.\\n2. **Color Crush Socks**: This name plays off the idea of having a crush on something, in this case, colorful socks!\\n3. **Toe Tales**: A playful name that references the stories your socks can tell (like where you\\'ve been and what adventures you\\'ve had).\\n4. **Sole Mates**: A clever name that references the idea that your socks are the perfect match for anyone\\'s feet.\\n5. **HueHub**: This name highlights the variety of colors your socks come in, positioning your company as a hub of colorful goodness!\\n6. **Socktastic**: A whimsical name that combines \"sock\" and \"fantastic\" to create a fun and playful vibe.\\n7. **Feet First**: A cute name that emphasizes the idea that your socks are the perfect way to start your day (or any outfit).\\n8. **Dye-Namic Socks**: This name references the vibrant colors of your socks, with a nod to the dynamic energy they bring.\\n9. **SockSations**: Another playful name that combines \"sock\" and \"sensations\" to create a fun and attention-grabbing vibe.\\n10. **Kaleidoscope Socks**: A name that references the colorful, swirly patterns found in kaleidoscopes, perfect for a company that offers a wide range of colorful sock designs.\\n\\nI hope one of these names sparks inspiration for your company!'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cSrTcSyw8Xrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 代码调用Ollama能输出，正确\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
        "from langchain.llms import Ollama\n",
        "\n",
        "# 下面是可用的\n",
        "# from langchain_community.chat_models import ChatZhipuAI\n",
        "# import os\n",
        "# import getpass\n",
        "# if os.getenv(\"ZHIPUAI_API_KEY\") is None:\n",
        "#   os.environ[\"ZHIPUAI_API_KEY\"] = getpass.getpass()\n",
        "#   api_key = os.getenv(\"ZHIPUAI_API_KEY\")\n",
        "\n",
        "# model_zhipu = ChatZhipuAI(\n",
        "#     model=\"glm-4\",\n",
        "#     temperature=0.5,\n",
        "#     model_kwargs={ \"response_format\": { \"type\": \"json_object\" } }\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# llM_ChatOpenAI = ChatOpenAI(\n",
        "#             temperature=0.95,\n",
        "#             model=\"glm-4\",\n",
        "#             openai_api_key=api_key,\n",
        "#             openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
        "#         )\n",
        "\n",
        "# llM_ChatOpenAI3报错，ChatOpenAI不能用\n",
        "# llM_ChatOpenAI3 = ChatOpenAI(\n",
        "#             temperature=0.95,\n",
        "#             model=\"llama3:8b\",\n",
        "#             openai_api_key='no api_key',\n",
        "#             openai_api_base=\"http://54.222.220.19:11435/api/chat\"\n",
        "#         )\n",
        "\n",
        "\n",
        "\n",
        "ollama_llm = Ollama(\n",
        "    base_url='http://54.222.220.19:11435'\n",
        "    ,model=\"llama3:8b\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "  \"Answer the user's question to the best of your ability.\"\n",
        "  'You must always output a JSON object with an \"answer\" key and a \"followup_question\" key.'\n",
        "  \"{question}\"\n",
        ")\n",
        "\n",
        "chain = prompt | ollama_llm\n",
        "\n",
        "chain.invoke({ \"question\": \"What is the powerhouse of the cell?\" })"
      ],
      "metadata": {
        "id": "kpLubnuX5Acq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "50d73aa4-039c-49b6-a690-eb5a22a75626"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n\"answer\": \"The powerhouse of the cell is the mitochondria.\",\\n\"followup_question\": \"What is the primary function of the mitochondria in the cell?\"\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#正确的调用JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "from langchain.llms import Ollama\n",
        "\n",
        "# 下面是可用的\n",
        "# from langchain_community.chat_models import ChatZhipuAI\n",
        "# import os\n",
        "# import getpass\n",
        "# if os.getenv(\"ZHIPUAI_API_KEY\") is None:\n",
        "#   os.environ[\"ZHIPUAI_API_KEY\"] = getpass.getpass()\n",
        "#   api_key = os.getenv(\"ZHIPUAI_API_KEY\")\n",
        "\n",
        "# model_zhipu = ChatZhipuAI(\n",
        "#     model=\"glm-4\",\n",
        "#     temperature=0.5,\n",
        "#     model_kwargs={ \"response_format\": { \"type\": \"json_object\" } }\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# llM_ChatOpenAI = ChatOpenAI(\n",
        "#             temperature=0.95,\n",
        "#             model=\"glm-4\",\n",
        "#             openai_api_key=api_key,\n",
        "#             openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
        "#         )\n",
        "\n",
        "# llM_ChatOpenAI3报错，ChatOpenAI不能用\n",
        "# llM_ChatOpenAI3 = ChatOpenAI(\n",
        "#             temperature=0.95,\n",
        "#             model=\"llama3:8b\",\n",
        "#             openai_api_key='no api_key',\n",
        "#             openai_api_base=\"http://54.222.220.19:11435/api/chat\"\n",
        "#         )\n",
        "\n",
        "\n",
        "\n",
        "ollama_llm = Ollama(\n",
        "    base_url='http://54.222.220.19:11435'\n",
        "    ,model=\"llama3:8b\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "  \"Answer the user's question to the best of your ability.\"\n",
        "  'You must always output a JSON object with an \"answer\" key and a \"followup_question\" key.'\n",
        "  \"{question}\"\n",
        ")\n",
        "\n",
        "chain = prompt | ollama_llm\n",
        "\n",
        "\n",
        "user_json = chain.invoke({ \"question\": \"What is the powerhouse of the cell?\" })\n",
        "output_parser = JsonOutputParser()\n",
        "\n",
        "# 使用输出解析器解析响应\n",
        "output_parser.parse(user_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58P6LcydAV8W",
        "outputId": "2de09098-3a0e-487b-ab11-4678ce895bb6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': \"The powerhouse of the cell is the mitochondria, which are often referred to as the 'powerhouses' or 'energy factories' because they generate most of the energy that the cell needs to function.\",\n",
              " 'followup_question': 'Would you like to know more about how mitochondria produce energy in cells?'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 逗号分隔列表输出解析器\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.llms import Ollama\n",
        "\n",
        "\n",
        "ollama_llm = Ollama(\n",
        "    base_url='http://54.222.220.19:11435'\n",
        "    ,model=\"llama3:8b\"\n",
        ")\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "# 创建提示模板，其中包含输出解析器的指令\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", f\"{output_parser.get_format_instructions()}\"),\n",
        "     (\"system\", \"请使用中文回复\"),\n",
        "    (\"human\", \"{user_input}\")\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "#直接手动输出结果进行测试\n",
        "response = \"中山陵,夫子庙,南京长江大桥\"\n",
        "parsed_response = output_parser.invoke(response)\n",
        "print(parsed_response)\n",
        "#输出：['中山陵', '夫子庙', '南京长江大桥']\n",
        "\n",
        "user_input = \"请列出南京的3个著名景点。\"\n",
        "chain = prompt | ollama_llm\n",
        "chain.invoke({user_input})\n",
        "#输出：'雨花台公园,中华门,玄武湖'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "kvkX8xs-EJzQ",
        "outputId": "909b6ae1-531d-43be-dad3-bf5536c89872"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['中山陵', '夫子庙', '南京长江大桥']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'雨花台公园,中华门,玄武湖'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lOWpEM1PG7Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cZjlej9J2d8g"
      }
    }
  ]
}